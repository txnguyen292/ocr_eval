[
  {
    "step": 1,
    "description": "Add question/answer extraction for DocVQA in loader/preview (use json['question'], json['answers']).",
    "status": "pending",
    "commands": [
      "edit src/ocr_eval/data/loader.py to include question/answer in samples for docvqa",
      "edit src/ocr_eval/utils/preview.py preview_docvqa_sample to print question/answer"
    ]
  },
  {
    "step": 2,
    "description": "Extend CLI metrics to include QA EM/F1 and cost/latency logging.",
    "status": "pending",
    "commands": [
      "edit src/ocr_eval/cli.py to compute EM/F1 using utils.metrics",
      "log tokens/latency per engine; include in results.md"
    ]
  },
  {
    "step": 3,
    "description": "Textract validation and clearer errors for creds/region before running.",
    "status": "pending",
    "commands": [
      "add AWS credential/region checks in src/ocr_eval/engines/textract.py or CLI",
      "fail fast with a helpful message if missing"
    ]
  },
  {
    "step": 4,
    "description": "Make temp/output paths configurable instead of fixed /tmp.",
    "status": "pending",
    "commands": [
      "add path options to load_dataset_samples and CLI evaluate",
      "update README with new flags"
    ]
  },
  {
    "step": 5,
    "description": "Add tests for loaders and metrics to catch schema drift.",
    "status": "pending",
    "commands": [
      "create tests for docvqa/funsd loaders under tests/",
      "run pytest"
    ]
  },
  {
    "step": 6,
    "description": "Add Textract bbox extraction + VLM bbox overlay notebook to compare boxes on the same DocVQA image.",
    "status": "pending",
    "commands": [
      "implement extract_text_with_boxes in TextractEngine and return pixel bboxes",
      "update notebooks/ocr_bbox_demo.ipynb to plot Textract vs VLM boxes side by side"
    ]
  },
  {
    "step": 7,
    "description": "Implement IoU-based bbox scoring (precision/recall/F1) plus text accuracy on matched boxes (exact match, CER/WER).",
    "status": "pending",
    "commands": [
      "add bbox matching and scoring helper in src/ocr_eval/utils/metrics.py",
      "wire it into the bbox demo notebook to print metrics"
    ]
  },
  {
    "step": 8,
    "description": "Document step-by-step project setup and structure in docs/progress_and_plan.md.",
    "status": "done",
    "commands": [
      "Prereqs: Python 3.12+, network access for HF datasets/OpenAI; AWS CLI optional for Textract",
      "mkdir ocr_eval && cd ocr_eval; python -m venv .venv && source .venv/bin/activate",
      "Create pyproject.toml with deps (boto3, datasets, huggingface_hub, ipykernel, matplotlib, numpy, openai, pandas, Pillow, python-Levenshtein, rapidfuzz, sacrebleu, typer, python-dotenv) and [tool.setuptools] pointing to src",
      "Create package skeleton under src/ocr_eval: __init__.py, config.py (env loader + DATASET_CONFIG), cli.py (typer evaluate), data/loader.py (DocVQA/FUNSD/CORD loaders saving temp images), engines/{base.py, textract.py, openai.py}, utils/{metrics.py, preview.py}, legacy_utils.py",
      "Add docs/progress_and_plan.md with progress + recreate steps; add docs/plan.json; add README.md describing datasets/CLI",
      "Add notebooks/ folder with docvqa.ipynb, funsd.ipynb (preview helpers) and optional bbox demo notebooks",
      "pip install -e . to register package; create .env with OPENAI_API_KEY, OPENAI_MODEL=gpt-4o, AWS_PROFILE or AWS keys, AWS_REGION, optional OCR_EVAL_TEMP_DIR",
      "Smoke test OpenAI OCR: python -m ocr_eval.cli evaluate --dataset docvqa --engine openai --samples 2 --output results.md",
      "Smoke test Textract: python -m ocr_eval.cli evaluate --dataset funsd --engine textract --samples 2 --output results.md"
    ]
  }
]
